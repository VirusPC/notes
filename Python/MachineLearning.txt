一.机器学习简介
1.

二.问题构建（Framing）：机器学习主要术语
1.（监督式）机器学习
    ·定义：机器学习系统通过学习如何组合输入信息来对从未见到过的数据做出有用的预测。

2.标签
    ·是我们要预测的事物，即即简单线性回归中的 y 变量。标签可以是小麦未来的价格、图片中
     显示的动物品种、音频剪辑的含义或任何事物。

3.特征
    ·特征是输入变量，即简单线性回归中的 x 变量。简单的机器学习项目可能会使用单个特征，
     而比较复杂的机器学习项目可能会使用数百万个特征，按如下方式指定：
        {x1, x2, ...xN}
    ·则垃圾邮件检测器示例中特征可能包括：
        ·电子邮件文本中的字词
        ·发件人的地址
        ·发送电子邮件的时段
        ·电子邮件中包含“一种奇怪的把戏”这样的短语。

4.样本
    ·样本是指数据的特定实例：x。（我们采用粗体 x 表示它是一个矢量。）我们将样本分为以下
     两类。
        ·有标签样本
        ·无标签样本
    ·有标签样本同时包含特征和标签。我们使用有标签样本来训练模型。在我们的垃圾邮件检测器
     示例中，有标签样本是用户明确标记为“垃圾邮件”或“非垃圾邮件”的各个电子邮件。
    ·无标签样本包含特征，但不包含标签。在使用有标签样本训练了我们的模型之后，我们会使用
     该模型来预测无标签样本的标签。

5.模型
    ·模型定义了特征与标签之间的关系。
    ·模型生命周期的两个阶段：
        ·训练表示创建或学习模型。也就是说，您向模型展示有标签样本，让模型主键学习特征与
         标签之间的关系。
        ·推断表示将训练后的模型来做出有用的预测 (y')。例如，在推断期间，您可以针对新的
         无标签样本预测 medianHouseValue。

6.回归与分类
    ·回归模型可预测连续值。例如，回归模型做出的预测可回答如下问题：
        ·加利福尼亚州一栋房产的价值是多少？
        ·用户点击此广告的概率是多少？
    ·分类模型可预测离散值。例如，分类模型做出的预测可回答如下问题：
        ·某个指定电子邮件是垃圾邮件还是非垃圾邮件？
        ·这是一张狗、猫还是仓鼠图片？


三.深入了解机器学习
1.线性回归
    ·蝉的鸣叫声与温度之间的关系的代数表示：
        y = mx + b
            · y 指的是温度（以摄氏度表示），即我们试图预测的值。
            · m 指的是直线的斜率。
            · x 指的是每分钟的鸣叫声次数，即输入特征的值。
            · b 指的是 y 轴截距。
    ·按照机器学习的惯例，您需要写一个存在细微差别的模型方程式：
        y' = b + w1x1
            · y 指的是预测标签（理想输出值）。
            · b 指的是偏差（y 轴截距）。而在一些机器学习文档中，它称为 w0。
            · w1 指的是特征 1 的权重。权重与上文中用 m 表示的“斜率”的概念相同。
            · x1 指的是特征（已知输入项）。
    ·要根据新的每分钟的鸣叫声值 x1 推断（预测）温度 y' ，只需将 x1 值代入此模型即可。
    ·下标（例如  和 ）预示着可以用多个特征来表示更复杂的模型。例如，具有三个特征的模型
     可以采用以下方程式：
        y' = b + w1x1 + w2x2 + w3x3

2.训练与损失
    ·训练模型：简单来说，训练模型表示通过有标签样本来学习（确定）所有权重和偏差的理想值
    ·经验风险最小化：监督式学习中，机器学习算法通过以下方式构建模型：检查多个样本并尝试
     找出可最大限度地减少损失的模型；这一过程称为经验风险最小化。
    ·损失：损失是对糟糕预测的惩罚。也就是说，损失是一个数值，表示对于单个样本而言模型预
     测的准确程度。如果模型的预测完全准确，则损失为零，否则损失会较大。
    ·目标：训练模型的目标是从所有样本中找到一组平均损失“较小”的权重和偏差。
    ·平方损失（L2损失）：一种常见的损失函数
        ·均方误差（MSE）指的是每个样本的平均平方损失。要计算 MSE，请求出各个样本的所有
         平方损失之和，然后除以样本数量：
        ·虽然 MSE 常用于机器学习，但它既不是唯一实用的损失函数，也不是适用于所有情形的
         最佳损失函数。

四.降低损失
1.迭代方法
    ·流程：
                 计算参数更新<---------|
                   |                  |
        特征--->模型(预测函数）----->  计算
        标签----------------------->  损失
    ·迭代策略在机器学习中的应用非常普遍，这主要是因为它们可以很好地扩展到大型数据集。
    ·“模型”部分将一个或多个特征作为输入，然后返回一个预测 (y') 作为输出。
        ·y' = b + w1x1 + w2x2 ... +wnxn
        ·对于线性回归问题，事实证明初始值并不重要。我们可以随机选择值。不过我们还是选择
         采用以下这些无关紧要的值：
            ·b = 0
            ·w1 = w2 = ... = wn = 0
    ·“计算损失”部分是模型将要使用的损失函数。假设我们使用平方损失函数。损失函数将
     采用两个输入值：
         ·y'：模型对特征 x 的预测
         ·y：特征 x 对应的正确标签。
    ·“计算参数更新”部分的作用是检查损失函数的值，并为 b 和 w1 生成新值。
    ·流程：计算参数更新部分会产生新值，然后机器学习系统将根据所有标签重新评估所有特征，
     为损失函数生成一个新值，而该值又产生新的参数值。这种学习过程会持续迭代，直到该算法
     发现损失可能最低的模型参数。通常，您可以不断迭代，直到总体损失不再变化或至少变化极
     其缓慢为止。这时候，我们可以说该模型已收敛。

2.梯度下降法
    ·迭代方法包含“计算参数更新”这一部分。现在，我们将用更实质的方法代替这一部分。
    ·损失函数的极点为损失函数收敛之处。但通过计算整个数据集中 w1 每个可能值的损失函数来
     找到收敛点这种方法效率太低。梯度下降法为一种更好的机制。
    ·梯度下降法的第一个阶段是为 w1 选择一个起始值（起点）。起点并不重要；因此很多算法就
     直接将 w1 设为 0 或随机选择一个值。
    ·然后，梯度下降法算法会计算损失曲线在起点处的梯度。简而言之，梯度是偏导数的矢量；它
     可以让您了解哪个方向距离目标“更近”或“更远”。损失相对于单个权重的梯度就等于导数。
    ·梯度是一个矢量，因此具有方向和大小两个特征。梯度始终指向损失函数中增长最为迅猛的方
     向。梯度下降法算法会沿着负梯度的方向走一步，以便尽快降低损失。
    ·为了确定损失函数曲线上的下一个点，梯度下降法算法会将梯度大小的一部分与起点相加。然
     后，梯度下降法会重复此过程，逐渐接近最低点。

3.学习速率
    ·梯度矢量具有方向和大小。梯度下降法算法用梯度乘以一个称为学习速率（有时也称为步长）
     的标量，以确定下一个点的位置。例如，如果梯度大小为 2.5，学习速率为 0.01，则梯度下
     降法算法会选择距离前一个点 0.025 的位置作为下一个点。
    ·超参数是编程人员在机器学习算法中用于调整的旋钮。大多数机器学习编程人员会花费相当多
     的时间来调整学习速率。如果您选择的学习速率过小，就会花费太长的学习时间。相反，如果
     您指定的学习速率过大，下一个点将永远在 U 形曲线的底部随意弹跳。如果您知道损失函数
     的梯度较小，则可以放心地试着采用更大的学习速率，以补偿较小的梯度并获得更大的步长。

4.随机梯度下降法
    ·在梯度下降法中，批量指的是用于在单次迭代中计算梯度的样本总数。
    ·包含随机抽样样本的大型数据集可能包含冗余数据。实际上，批量大小越大，出现冗余的可能
     性就越高。一些冗余可能有助于消除杂乱的梯度，但超大批量所具备的预测价值往往并不比大
     型批量高。
    ·通过从我们的数据集中随机选择样本，我们可以通过小得多的数据集估算（尽管过程非常杂乱
     ）出较大的平均值。 随机梯度下降法 (SGD) 将这种想法运用到极致，它每次迭代只使用一
     个样本（批量大小为 1）。如果进行足够的迭代，SGD 也可以发挥作用，但过程会非常杂乱。
     “随机”这一术语表示构成各个批量的一个样本都是随机选择的。
    ·小批量随机梯度下降法（小批量 SGD）是介于全批量迭代与 SGD 之间的折衷方案。小批量通
     常包含 10-1000 个随机选择的样本。小批量 SGD

五.使用TF的基本步骤
1.工具包
    ·层次结构
              TensorFlow Estimators       <----面向对象的高级API
           tf.layers/tf.losses/tf.metrics  <----用于常见模型组件的可重用库
                 TensorFlow Python         <----提供可封装C++内核的指令
                  TensorFlow C++
                   CPU  GPU  TPU           <----内核在一个或多个平台上运行
    ·不同层的用途
        工具包                             说明
        Estimator(tf.estimator)           高级OOP API
        tf.layers/tf.losses/tf.metrics    用于常见模型组件的库
        tensorflow                        低级API

2.TensorFlow
    ·组成：
        ·图协议缓冲区
        ·执行（分布式）图的运行时
    ·这两个组件类似于Java编译器和 JVM。正如 JVM 会实施在多个硬件平台（CPU 和 GPU）上
     一样，TensorFlow 也是如此。

3.tf.estimator API
    ·相对于在较低级别（原始）的 TensorFlow 中完成代码，使用 tf.estimator 会大大减少
     代码行数。
    ·scikit-learn 是极其热门的 Python 开放源代码机器学习库。tf.estimator 与
     scikit-learn API 兼容。
    ·以下是在 tf.estimator 中实现的线性回归程序的格式：
        import tensorflow as tf

        # Set up a linear classifier.
        classifier = tf.estimator.LinearClassifier()

        # Train the model on some example data.
        classifier.train(input_fn=train_input_fn, steps=2000)

        # Use it to predict.
        predictions = classifier.predict(input_fn=predict_input_fn)

六.泛化
七.训练集和测试集
八.验证
九.表示法
十.特征组合
十一.正则化:简单性
十二.逻辑回归
十三.分类
十四.正则化:洗属性
十五.神经网络简介
十六.训练神经网络
十七.多类别神经网络
十八.嵌入
